{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fr4FQgq7T5PD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loan Eligibility Prediction with LIME Explanations\n",
        "\n",
        "This notebook demonstrates building a machine learning model to predict loan eligibility and then uses LIME (Local Interpretable Model-agnostic Explanations) to understand the model's decisions for individual applications.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Setup:** Install necessary libraries and configure Kaggle API access.\n",
        "2.  **Data Loading:** Load the loan eligibility dataset from Kaggle Hub.\n",
        "3.  **Data Cleaning & Preparation:** Clean the data, handle missing values, and transform the target variable.\n",
        "4.  **Preprocessing:** Define and apply preprocessing steps for numerical and categorical features using `ColumnTransformer`.\n",
        "5.  **Train-Test Split:** Split the data into training and testing sets.\n",
        "6.  **Model Training:** Train a Gradient Boosting Classifier.\n",
        "7.  **Model Evaluation:** Evaluate the model's performance.\n",
        "8.  **XAI with LIME:**\n",
        "    *   Set up the LIME explainer.\n",
        "    *   Develop a function to explain individual predictions.\n",
        "    *   Demonstrate explanations for sample approved and rejected cases.\n",
        "\n",
        "Let's start by installing the required packages."
      ],
      "metadata": {
        "id": "Atolb85vYOYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup: Install Libraries\n",
        "We need to install `kagglehub` for dataset access, `kaggle` for the API, `lime` for explainability, and standard data science libraries like `scikit-learn`, `pandas`, and `numpy`. `scipy` is also included as LIME might use it internally."
      ],
      "metadata": {
        "id": "j55zB-fBZlnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub kaggle lime scikit-learn pandas numpy scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_6Dad97YNzz",
        "outputId": "a191a837-9de8-481e-907f-742fac5595cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.11/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.5.21)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configure Kaggle API Access\n",
        "To download datasets from Kaggle using `kagglehub` or the `kaggle` CLI, we need to authenticate. This cell configures Kaggle API access using credentials stored in Colab Secrets.\n",
        "\n",
        "**Before running this, ensure you have:**\n",
        "1. Added your Kaggle username as a Colab Secret named `KAGGLE_USERNAME`.\n",
        "2. Added your Kaggle API key (from your `kaggle.json` file) as a Colab Secret named `KAGGLE_KEY`."
      ],
      "metadata": {
        "id": "NS7u-ZXfZoJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# Create the .kaggle directory and kaggle.json\n",
        "# This is often necessary for the Kaggle API tools to find the credentials.\n",
        "if not os.path.exists(\"/content/.kaggle\"):\n",
        "    os.makedirs(\"/content/.kaggle\")\n",
        "\n",
        "kaggle_json_content = f'{{\"username\":\"{os.environ[\"KAGGLE_USERNAME\"]}\",\"key\":\"{os.environ[\"KAGGLE_KEY\"]}\"}}'\n",
        "with open(\"/content/.kaggle/kaggle.json\", \"w\") as f:\n",
        "    f.write(kaggle_json_content)\n",
        "os.chmod(\"/content/.kaggle/kaggle.json\", 600) # Set appropriate permissions\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/.kaggle\" # Point Kaggle tools to this directory\n",
        "\n",
        "print(\"Kaggle credentials configured from Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npiq0lKzYXN3",
        "outputId": "4f13462c-9051-45be-925b-4e6d0aa17055"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials configured from Colab Secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Import Libraries and Load Data\n",
        "\n",
        "Now we import all necessary Python libraries and load the \"Loan Eligible Dataset\" from Kaggle Hub. We'll be using the `loan-train.csv` file as it contains the target variable `Loan_Status`.\n",
        "\n",
        "The `KaggleDatasetAdapter.PANDAS` is explicitly used with `kagglehub.load_dataset` to ensure the data is loaded directly as a Pandas DataFrame."
      ],
      "metadata": {
        "id": "wlV2kyWgZtaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import warnings\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter # Import KaggleDatasetAdapter\n",
        "import scipy # Import scipy\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# --- 1. Load Data from Kaggle Hub ---\n",
        "print(\"--- Loading dataset from Kaggle Hub ---\")\n",
        "try:\n",
        "    df = kagglehub.load_dataset(\n",
        "        handle=\"vikasukani/loan-eligible-dataset\",\n",
        "        path=\"loan-train.csv\", # Specify the training file\n",
        "        adapter=KaggleDatasetAdapter.PANDAS # Explicitly add the adapter\n",
        "    )\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"First 5 records:\", df.head())\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset from Kaggle Hub: {e}\")\n",
        "    print(\"Please ensure:\")\n",
        "    print(\"1. You have 'kagglehub' and 'kaggle' installed.\")\n",
        "    print(\"2. Your Kaggle API token is set up correctly in Colab Secrets and configured via the code above.\")\n",
        "    raise # Re-raise the exception to prevent NameError later"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBUgVw0ZYenl",
        "outputId": "94a87f34-013d-492c-f1ea-d2c43a34ab31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading dataset from Kaggle Hub ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-b5a4767cde19>:24: DeprecationWarning: load_dataset is deprecated and will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "First 5 records:     Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
            "0  LP001002   Male      No          0      Graduate            No   \n",
            "1  LP001003   Male     Yes          1      Graduate            No   \n",
            "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
            "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
            "4  LP001008   Male      No          0      Graduate            No   \n",
            "\n",
            "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
            "0             5849                0.0         NaN             360.0   \n",
            "1             4583             1508.0       128.0             360.0   \n",
            "2             3000                0.0        66.0             360.0   \n",
            "3             2583             2358.0       120.0             360.0   \n",
            "4             6000                0.0       141.0             360.0   \n",
            "\n",
            "   Credit_History Property_Area Loan_Status  \n",
            "0             1.0         Urban           Y  \n",
            "1             1.0         Rural           N  \n",
            "2             1.0         Urban           Y  \n",
            "3             1.0         Urban           Y  \n",
            "4             1.0         Urban           Y  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Initial Data Cleaning & Target Variable Preparation\n",
        "\n",
        "In this step, we perform some initial data cleaning:\n",
        "*   Create a copy of the original DataFrame (`df_orig_for_lime`) which will be used later by LIME to show explanations with original feature values.\n",
        "*   Drop the `Loan_ID` column as it's an identifier and not useful for model training.\n",
        "*   Map the target variable `Loan_Status` from 'Y'/'N' to 1/0.\n",
        "*   Handle the `Dependents` column: replace '3+' with '3' and convert to a numeric type.\n",
        "*   Separate features (X) and the target variable (y)."
      ],
      "metadata": {
        "id": "DzBFYShVaBIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Initial Data Cleaning & Target Variable Preparation ---\n",
        "print(\"\\n--- Initial Data Cleaning & Target Preparation ---\")\n",
        "df_orig_for_lime = df.copy() # Keep a version for LIME with original string values\n",
        "\n",
        "# Drop Loan_ID as it's an identifier\n",
        "if 'Loan_ID' in df.columns:\n",
        "    df = df.drop('Loan_ID', axis=1)\n",
        "if 'Loan_ID' in df_orig_for_lime.columns:\n",
        "    df_orig_for_lime = df_orig_for_lime.drop('Loan_ID', axis=1)\n",
        "\n",
        "\n",
        "# Convert target variable 'Loan_Status' (Y/N) to binary (1/0)\n",
        "if 'Loan_Status' not in df.columns:\n",
        "    print(\"Error: 'Loan_Status' column not found in the loaded dataset.\")\n",
        "    raise ValueError(\"'Loan_Status' column not found in the loaded dataset.\")\n",
        "\n",
        "\n",
        "df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})\n",
        "df_orig_for_lime['Loan_Status'] = df_orig_for_lime['Loan_Status'].map({'Y': 1, 'N': 0})\n",
        "\n",
        "print(f\"Loan_Status distribution:\\n{df['Loan_Status'].value_counts(normalize=True)}\")\n",
        "\n",
        "# Handle '3+' in Dependents\n",
        "if 'Dependents' in df.columns:\n",
        "    df['Dependents'] = df['Dependents'].replace('3+', '3')\n",
        "    df_orig_for_lime['Dependents'] = df_orig_for_lime['Dependents'].replace('3+', '3')\n",
        "    df['Dependents'] = pd.to_numeric(df['Dependents'], errors='coerce')\n",
        "    df_orig_for_lime['Dependents'] = pd.to_numeric(df_orig_for_lime['Dependents'], errors='coerce')\n",
        "\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('Loan_Status', axis=1)\n",
        "y = df['Loan_Status']\n",
        "\n",
        "# X_orig_lime will be used for LIME explainer's training_data and for explaining instances\n",
        "# It should not contain the target variable.\n",
        "X_orig_lime = df_orig_for_lime.drop('Loan_Status', axis=1, errors='ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJOPXMeIYj1M",
        "outputId": "eaed54c8-6d77-4db9-a285-d36653a5ad6e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initial Data Cleaning & Target Preparation ---\n",
            "Loan_Status distribution:\n",
            "Loan_Status\n",
            "1    0.687296\n",
            "0    0.312704\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preprocessing Setup\n",
        "\n",
        "We define how different types of features will be preprocessed.\n",
        "*   **Numerical Features:** Imputed with the median value and then scaled using `StandardScaler`.\n",
        "*   **Categorical Features (Ordinal):** These are typically binary or have a natural order after imputation (e.g., 'Gender', 'Married'). They are imputed with the most frequent value and then encoded using `OrdinalEncoder`.\n",
        "*   **Categorical Features (One-Hot Encoded):** These are nominal categorical features with multiple categories (e.g., 'Property_Area'). They are imputed with the most frequent value and then transformed using `OneHotEncoder`.\n",
        "\n",
        "A `ColumnTransformer` is used to apply these different transformations to the appropriate columns."
      ],
      "metadata": {
        "id": "n-3uVRhNaOJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Preprocessing using ColumnTransformer ---\n",
        "print(\"\\n--- Setting up Preprocessing ---\")\n",
        "\n",
        "numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n",
        "if 'Dependents' in X.columns and pd.api.types.is_numeric_dtype(X['Dependents']):\n",
        "    if 'Dependents' not in numerical_features:\n",
        "        numerical_features.append('Dependents')\n",
        "if 'Credit_History' in X.columns and pd.api.types.is_numeric_dtype(X['Credit_History']):\n",
        "     if 'Credit_History' not in numerical_features:\n",
        "        numerical_features.append('Credit_History')\n",
        "\n",
        "categorical_features_for_ohe = []\n",
        "categorical_features_for_ordinal = []\n",
        "for col in X.columns:\n",
        "    if col not in numerical_features:\n",
        "        if X[col].nunique() > 2 and X[col].dtype == 'object':\n",
        "             categorical_features_for_ohe.append(col)\n",
        "        elif X[col].dtype == 'object':\n",
        "            categorical_features_for_ordinal.append(col)\n",
        "\n",
        "print(f\"Numerical features: {numerical_features}\")\n",
        "print(f\"Categorical (Ordinal): {categorical_features_for_ordinal}\")\n",
        "print(f\"Categorical (OHE): {categorical_features_for_ohe}\")\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "ordinal_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "])\n",
        "ohe_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "transformers_list = []\n",
        "if numerical_features:\n",
        "    transformers_list.append(('num', numerical_transformer, numerical_features))\n",
        "if categorical_features_for_ordinal:\n",
        "    transformers_list.append(('ord', ordinal_transformer, categorical_features_for_ordinal))\n",
        "if categorical_features_for_ohe:\n",
        "    transformers_list.append(('ohe', ohe_transformer, categorical_features_for_ohe))\n",
        "\n",
        "if not transformers_list:\n",
        "    print(\"Error: No features identified for preprocessing. Check feature lists.\")\n",
        "    raise ValueError(\"No features identified for preprocessing. Check feature lists.\")\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=transformers_list, remainder='drop')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVQqA1U7YoGF",
        "outputId": "2951cd54-0d85-4066-f4a3-d5943d008080"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Setting up Preprocessing ---\n",
            "Numerical features: ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Dependents', 'Credit_History']\n",
            "Categorical (Ordinal): ['Gender', 'Married', 'Education', 'Self_Employed']\n",
            "Categorical (OHE): ['Property_Area']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train-Test Split\n",
        "\n",
        "The dataset is split into training and testing sets. We use `X_orig_lime` (features before ColumnTransformer processing) for this split because LIME will need to see feature values in their original, interpretable form. The target `y` is also split accordingly. `stratify=y` ensures that the proportion of the target classes is maintained in both splits."
      ],
      "metadata": {
        "id": "lxq86w0KaRiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Train-Test Split ---\n",
        "X_train_orig, X_test_orig, y_train, y_test = train_test_split(\n",
        "    X_orig_lime, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Shape of X_train_orig: {X_train_orig.shape}\")\n",
        "print(f\"Shape of X_test_orig: {X_test_orig.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlA9nX-TYsgm",
        "outputId": "75f1812f-06db-434a-b207-742b679989f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_orig: (460, 11)\n",
            "Shape of X_test_orig: (154, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Fit Preprocessor and Transform Data\n",
        "\n",
        "The `preprocessor` (ColumnTransformer) defined earlier is now:\n",
        "1.  **Fitted** on the training data (`X_train_orig`). This means the imputers learn the statistics (median, mode) and the scalers/encoders learn their parameters *only from the training data* to prevent data leakage.\n",
        "2.  Used to **transform** both the training data (`X_train_orig`) and the test data (`X_test_orig`) into their processed versions (`X_train_processed`, `X_test_processed`). These processed versions are what the model will be trained and evaluated on."
      ],
      "metadata": {
        "id": "VQy6x_7AaZva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Fit Preprocessor and Transform Data ---\n",
        "print(\"\\n--- Fitting Preprocessor and Transforming Data ---\")\n",
        "preprocessor.fit(X_train_orig)\n",
        "\n",
        "X_train_processed = preprocessor.transform(X_train_orig)\n",
        "X_test_processed = preprocessor.transform(X_test_orig)\n",
        "\n",
        "try:\n",
        "    feature_names_out = preprocessor.get_feature_names_out()\n",
        "    print(f\"Number of features after processing: {len(feature_names_out)}\")\n",
        "    # print(f\"Processed feature names: {feature_names_out[:15]}...\") # Print a sample\n",
        "except Exception as e:\n",
        "    print(f\"Could not get feature names out automatically. Error: {e}\")\n",
        "    feature_names_out = None\n",
        "\n",
        "print(f\"Shape of X_train_processed: {X_train_processed.shape}\")\n",
        "print(f\"Shape of X_test_processed: {X_test_processed.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDQBRAqDYwiJ",
        "outputId": "144442a1-a552-49a8-a919-4c2f1a1a060c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fitting Preprocessor and Transforming Data ---\n",
            "Number of features after processing: 13\n",
            "Shape of X_train_processed: (460, 13)\n",
            "Shape of X_test_processed: (154, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Train the Machine Learning Model\n",
        "\n",
        "We'll use a `GradientBoostingClassifier` for this loan prediction task. It's a powerful ensemble model that often provides good performance. The model is trained on the `X_train_processed` data and `y_train` labels."
      ],
      "metadata": {
        "id": "x4ufksmSae79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Train the Model ---\n",
        "print(\"\\n--- Training the Model ---\")\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model.fit(X_train_processed, y_train)\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jctFCre1Yz7o",
        "outputId": "ce179f3d-1879-4d4a-9597-5a9899a303f4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training the Model ---\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Evaluate the Model\n",
        "\n",
        "The trained model is evaluated on the `X_test_processed` data. We'll look at accuracy and a detailed classification report (precision, recall, F1-score)."
      ],
      "metadata": {
        "id": "po28I8nNahKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Evaluate the Model ---\n",
        "y_pred = model.predict(X_test_processed)\n",
        "print(\"\\n--- Model Evaluation (on Real Labeled Data) ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Rejected (0)', 'Approved (1)']))\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-9y2HMgY3H_",
        "outputId": "7c6f9808-b46d-4250-a49d-beb6d1c8c9ac"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Evaluation (on Real Labeled Data) ---\n",
            "Accuracy: 0.8247\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Rejected (0)       0.82      0.56      0.67        48\n",
            "Approved (1)       0.83      0.94      0.88       106\n",
            "\n",
            "    accuracy                           0.82       154\n",
            "   macro avg       0.82      0.75      0.77       154\n",
            "weighted avg       0.82      0.82      0.81       154\n",
            "\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. XAI with LIME: Setup\n",
        "\n",
        "Now we set up the LIME (Local Interpretable Model-agnostic Explanations) component.\n",
        "\n",
        "**Key LIME Components:**\n",
        "*   **`predict_fn_for_lime`:** A crucial function that LIME uses. It takes perturbed data samples (in their original, un-transformed format) from LIME, applies the *entire preprocessing pipeline* (the same one used for training), and then returns the model's probability predictions for these processed samples.\n",
        "*   **`LimeTabularExplainer`:** The main LIME object.\n",
        "    *   `training_data`: LIME uses this to understand the distribution of features and to generate meaningful perturbations. It needs to be numeric for LIME's internal statistics, so we create a temporarily numerically-encoded version (`X_train_orig_numeric_for_lime_stats`) for this argument. String features are converted to category codes.\n",
        "    *   `feature_names`: Original names of the features.\n",
        "    *   `class_names`: Names for the target classes (e.g., 'Rejected', 'Approved').\n",
        "    *   `categorical_features`: Indices of columns that LIME should treat as categorical when perturbing.\n",
        "    *   `mode`: 'classification' for this task.\n",
        "    *   `discretize_continuous=False`: This was changed from `True`. When `True`, LIME attempts to discretize continuous features into bins, which sometimes can lead to errors with `scipy.stats.truncnorm` if the data distribution is unusual after LIME's internal scaling of the provided `training_data` (especially if it contains strings that get converted oddly by LIME's internal standard scaler). Setting it to `False` makes LIME treat continuous features as is (or rely on its default perturbation for them), potentially avoiding the error. LIME can still provide meaningful explanations for continuous features by showing their direct impact.\n",
        "\n",
        "**Handling Categorical Data for LIME's `training_data`:**\n",
        "LIME's `LimeTabularExplainer` expects the `training_data` argument (used for internal statistics and sampling) to be numeric. If your original training data (`X_train_orig`) contains string-based categorical features, passing it directly can cause errors in LIME's internal scaler.\n",
        "To address this, we:\n",
        "1. Create `X_train_orig_numeric_for_lime_stats` by copying `X_train_orig`.\n",
        "2. In this copy, we convert string categorical columns to their pandas category codes (`.astype('category').cat.codes`). This provides a numeric representation.\n",
        "3. We pass this `X_train_orig_numeric_for_lime_stats.values` to `LimeTabularExplainer`.\n",
        "4. Crucially, the `predict_fn_for_lime` still expects and processes data in the *original format* (with strings for categoricals), and our `preprocessor` handles the actual encoding for the model.\n",
        "5. The `data_row` passed to `explainer.explain_instance` must also be numerically encoded in the same way as `X_train_orig_numeric_for_lime_stats` for consistency with LIME's internal state."
      ],
      "metadata": {
        "id": "bGIeScYwamw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. XAI Component using LIME ---\n",
        "print(\"\\n--- Setting up LIME Explainer ---\")\n",
        "\n",
        "def predict_fn_for_lime(data_lime_numpy):\n",
        "    data_lime_df = pd.DataFrame(data_lime_numpy, columns=X_train_orig.columns)\n",
        "    for col in X_train_orig.columns:\n",
        "         if pd.api.types.is_numeric_dtype(X_train_orig[col].dtype):\n",
        "             data_lime_df[col] = pd.to_numeric(data_lime_df[col], errors='coerce')\n",
        "    data_processed = preprocessor.transform(data_lime_df)\n",
        "    return model.predict_proba(data_processed)\n",
        "\n",
        "lime_categorical_feature_names = categorical_features_for_ordinal + categorical_features_for_ohe\n",
        "lime_categorical_features_indices = [\n",
        "    X_train_orig.columns.get_loc(col) for col in X_train_orig.columns\n",
        "    if col in lime_categorical_feature_names\n",
        "]\n",
        "lime_categorical_features_indices = sorted(list(set(lime_categorical_features_indices)))\n",
        "\n",
        "X_train_orig_numeric_for_lime_stats = X_train_orig.copy()\n",
        "categorical_mappings = {} # To store mappings for encoding instances later\n",
        "for col in X_train_orig_numeric_for_lime_stats.columns:\n",
        "    if X_train_orig_numeric_for_lime_stats[col].dtype == 'object':\n",
        "         # Ensure NaN values are handled before converting to category codes\n",
        "         # Pandas cat.codes handles NaNs by assigning -1 by default\n",
        "         X_train_orig_numeric_for_lime_stats[col] = X_train_orig_numeric_for_lime_stats[col].astype('category')\n",
        "         categorical_mappings[col] = dict(enumerate(X_train_orig_numeric_for_lime_stats[col].cat.categories))\n",
        "         X_train_orig_numeric_for_lime_stats[col] = X_train_orig_numeric_for_lime_stats[col].cat.codes\n",
        "\n",
        "\n",
        "print(f\"LIME categorical feature indices (on original columns): {lime_categorical_features_indices}\")\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train_orig_numeric_for_lime_stats.values,\n",
        "    feature_names=X_train_orig.columns.tolist(),\n",
        "    class_names=['Rejected', 'Approved'],\n",
        "    categorical_features=lime_categorical_features_indices,\n",
        "    mode='classification',\n",
        "    discretize_continuous=False # Changed to False\n",
        ")\n",
        "print(\"LIME explainer setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFIAoIjbY9Jc",
        "outputId": "50b2be00-ea96-461d-9388-042fa8dbdef2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Setting up LIME Explainer ---\n",
            "LIME categorical feature indices (on original columns): [0, 1, 3, 4, 10]\n",
            "LIME explainer setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. XAI with LIME: Explain Individual Decisions\n",
        "\n",
        "This function, `explain_instance_lime`, takes a specific instance from the test set, gets the model's prediction, and then uses LIME to generate an explanation.\n",
        "\n",
        "**Important:** The `data_row` passed to `explainer.explain_instance` must be numerically encoded in the same way as the `training_data` provided to the `LimeTabularExplainer`. We use the same `.astype('category').cat.codes` logic for the instance being explained to achieve this consistency.\n",
        "\n",
        "The explanation shows:\n",
        "*   The applicant's original data.\n",
        "*   The model's prediction and probability.\n",
        "*   A human-readable summary of the top factors influencing the decision, based on LIME weights."
      ],
      "metadata": {
        "id": "XSnCRnsparQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. Explain a Specific Decision ---\n",
        "import re # For robust feature name parsing\n",
        "\n",
        "def explain_instance_lime(instance_index_in_test_orig, X_test_orig_df, model, explainer, preprocessor_pipeline, categorical_mappings, num_features_to_show=7):\n",
        "    instance_orig_series = X_test_orig_df.iloc[instance_index_in_test_orig]\n",
        "\n",
        "    print(f\"\\n--- Explaining Instance (Original Test Index: {instance_index_in_test_orig}, DataFrame Index: {instance_orig_series.name}) ---\")\n",
        "    print(\"Applicant's Original Data (before model processing):\")\n",
        "    for col, val in instance_orig_series.items():\n",
        "        print(f\"  {col}: {val}\")\n",
        "\n",
        "    instance_for_prediction_df = instance_orig_series.to_frame().T\n",
        "    instance_for_prediction_df = instance_for_prediction_df[X_train_orig.columns] # Ensure column order\n",
        "\n",
        "    instance_processed = preprocessor_pipeline.transform(instance_for_prediction_df)\n",
        "    prediction_proba = model.predict_proba(instance_processed)\n",
        "    prediction = model.predict(instance_processed)[0]\n",
        "    predicted_class = 'Approved' if prediction == 1 else 'Rejected'\n",
        "\n",
        "    print(f\"\\nModel Prediction: {predicted_class} (Prob Approved: {prediction_proba[0,1]:.2f}, Prob Rejected: {prediction_proba[0,0]:.2f})\")\n",
        "\n",
        "    # Numerically encode the instance for LIME's explain_instance data_row\n",
        "    instance_numeric_for_lime_stats_df = instance_orig_series.to_frame().T.copy()\n",
        "    instance_numeric_for_lime_stats_df = instance_numeric_for_lime_stats_df[X_train_orig.columns] # Ensure column order\n",
        "\n",
        "    for col in instance_numeric_for_lime_stats_df.columns:\n",
        "        if X_train_orig[col].dtype == 'object': # Check original dtype to decide if encoding is needed\n",
        "            # Reverse lookup: find code for the instance's value using the mapping\n",
        "            # This assumes categorical_mappings stores {code: category_value} or we need to invert it\n",
        "            # Let's adjust categorical_mappings to be {category_value: code} for easier lookup\n",
        "            # Or, better, stick to the .astype('category').cat.codes approach for the instance too.\n",
        "\n",
        "            temp_series = instance_numeric_for_lime_stats_df[col].astype('category')\n",
        "            # Ensure categories are consistent with training data if possible\n",
        "            if col in categorical_mappings: # Check if this column had a mapping from training\n",
        "                 # Re-apply categories from training data to handle new/missing values in instance consistently\n",
        "                 # This can be tricky if instance has values not in training categories for that column\n",
        "                 # For simplicity, we'll use the instance's own categories for now,\n",
        "                 # but a more robust solution would align with training categories.\n",
        "                 # A simpler way: Use the same logic as for training_data\n",
        "                 try:\n",
        "                     # Use the categories from X_train_orig if available\n",
        "                     trained_categories = X_train_orig[col].astype('category').cat.categories\n",
        "                     instance_numeric_for_lime_stats_df[col] = pd.Categorical(instance_numeric_for_lime_stats_df[col], categories=trained_categories).codes\n",
        "                 except Exception: # Fallback if categories don't match\n",
        "                     instance_numeric_for_lime_stats_df[col] = instance_numeric_for_lime_stats_df[col].astype('category').cat.codes\n",
        "            else: # If no specific mapping (e.g. not an object type in training)\n",
        "                 instance_numeric_for_lime_stats_df[col] = instance_numeric_for_lime_stats_df[col].astype('category').cat.codes\n",
        "\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        data_row=instance_numeric_for_lime_stats_df.iloc[0].values,\n",
        "        predict_fn=predict_fn_for_lime,\n",
        "        num_features=num_features_to_show,\n",
        "        top_labels=1\n",
        "    )\n",
        "\n",
        "    print(\"\\nLIME Explanation (Top Factors contributing to the prediction):\")\n",
        "    explained_label_index = explanation.available_labels()[0]\n",
        "    explained_label_name = explainer.class_names[explained_label_index]\n",
        "    print(f\"Explanation for why the model predicted: '{explained_label_name}'\")\n",
        "\n",
        "    human_readable_explanation = f\"The model predicted '{predicted_class}' for applicant (index {instance_orig_series.name}) primarily because:\\n\"\n",
        "    for feature_condition, weight in explanation.as_list(label=explained_label_index):\n",
        "        match = re.match(r'([^<=>]+)\\s*[<=>]?.*', feature_condition)\n",
        "        base_feature_name = match.group(1).strip() if match else feature_condition.split(' ')[0].split('=')[0].split('<')[0].split('>')[0].strip()\n",
        "        try:\n",
        "            actual_value = instance_orig_series[base_feature_name]\n",
        "            value_str = f\"(Applicant's value for {base_feature_name}: {actual_value})\"\n",
        "        except KeyError: value_str = \"\"\n",
        "        influence_verb = \"supported\" if weight > 0 else \"opposed\"\n",
        "        strength_adv = \"strongly\" if abs(weight) > 0.1 else \"moderately\" if abs(weight) > 0.05 else \"slightly\"\n",
        "        human_readable_explanation += (f\"- The condition '{feature_condition}' {value_str} \"\n",
        "                                       f\"{strength_adv} {influence_verb} the '{explained_label_name}' decision (LIME weight: {weight:.3f}).\\n\")\n",
        "    print(\"\\n--- Human-Understandable Explanation ---\")\n",
        "    print(human_readable_explanation)\n",
        "\n",
        "    # For interactive LIME plots in Colab/Jupyter (optional)\n",
        "    # from IPython.display import display, HTML\n",
        "    # display(HTML(explanation.as_html()))\n",
        "    # Or: explanation.show_in_notebook(show_table=True, show_all=False)\n",
        "    return explanation\n",
        "\n",
        "# --- Example Usage: Explain a few instances from the original test set ---\n",
        "if not X_test_orig.empty:\n",
        "    approved_pred_indices_test = [i for i, p_val in enumerate(y_pred) if p_val == 1]\n",
        "    if approved_pred_indices_test:\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "        print(\"EXPLAINING AN APPROVED CASE:\")\n",
        "        explain_instance_lime(approved_pred_indices_test[0], X_test_orig, model, explainer, preprocessor, categorical_mappings)\n",
        "    else:\n",
        "        print(\"\\nNo instances predicted as 'Approved' in the test set to explain.\")\n",
        "\n",
        "    rejected_pred_indices_test = [i for i, p_val in enumerate(y_pred) if p_val == 0]\n",
        "    if rejected_pred_indices_test:\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "        print(\"EXPLAINING A REJECTED CASE:\")\n",
        "        explain_instance_lime(rejected_pred_indices_test[0], X_test_orig, model, explainer, preprocessor, categorical_mappings)\n",
        "    else:\n",
        "        print(\"\\nNo instances predicted as 'Rejected' in the test set to explain.\")\n",
        "else:\n",
        "    print(\"Test set is empty. Cannot generate explanations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWNhXoZzY_Hg",
        "outputId": "3cae0e6c-0876-4177-e5bc-f716816017e8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "\n",
            "EXPLAINING AN APPROVED CASE:\n",
            "\n",
            "--- Explaining Instance (Original Test Index: 0, DataFrame Index: 194) ---\n",
            "Applicant's Original Data (before model processing):\n",
            "  Gender: Male\n",
            "  Married: No\n",
            "  Dependents: 0.0\n",
            "  Education: Graduate\n",
            "  Self_Employed: No\n",
            "  ApplicantIncome: 4191\n",
            "  CoapplicantIncome: 0.0\n",
            "  LoanAmount: 120.0\n",
            "  Loan_Amount_Term: 360.0\n",
            "  Credit_History: 1.0\n",
            "  Property_Area: Rural\n",
            "\n",
            "Model Prediction: Approved (Prob Approved: 0.79, Prob Rejected: 0.21)\n",
            "\n",
            "LIME Explanation (Top Factors contributing to the prediction):\n",
            "Explanation for why the model predicted: 'Approved'\n",
            "\n",
            "--- Human-Understandable Explanation ---\n",
            "The model predicted 'Approved' for applicant (index 194) primarily because:\n",
            "- The condition 'ApplicantIncome' (Applicant's value for ApplicantIncome: 4191) strongly supported the 'Approved' decision (LIME weight: 0.121).\n",
            "- The condition 'Credit_History' (Applicant's value for Credit_History: 1.0) strongly supported the 'Approved' decision (LIME weight: 0.102).\n",
            "- The condition 'Loan_Amount_Term' (Applicant's value for Loan_Amount_Term: 360.0) slightly opposed the 'Approved' decision (LIME weight: -0.049).\n",
            "- The condition 'Dependents' (Applicant's value for Dependents: 0.0) slightly supported the 'Approved' decision (LIME weight: 0.022).\n",
            "- The condition 'Property_Area=0' (Applicant's value for Property_Area: Rural) slightly opposed the 'Approved' decision (LIME weight: -0.018).\n",
            "- The condition 'CoapplicantIncome' (Applicant's value for CoapplicantIncome: 0.0) slightly supported the 'Approved' decision (LIME weight: 0.011).\n",
            "- The condition 'Gender=1' (Applicant's value for Gender: Male) slightly supported the 'Approved' decision (LIME weight: 0.006).\n",
            "\n",
            "\n",
            "============================================================\n",
            "\n",
            "EXPLAINING A REJECTED CASE:\n",
            "\n",
            "--- Explaining Instance (Original Test Index: 5, DataFrame Index: 546) ---\n",
            "Applicant's Original Data (before model processing):\n",
            "  Gender: Male\n",
            "  Married: No\n",
            "  Dependents: 0.0\n",
            "  Education: Not Graduate\n",
            "  Self_Employed: No\n",
            "  ApplicantIncome: 3358\n",
            "  CoapplicantIncome: 0.0\n",
            "  LoanAmount: 80.0\n",
            "  Loan_Amount_Term: 36.0\n",
            "  Credit_History: 1.0\n",
            "  Property_Area: Semiurban\n",
            "\n",
            "Model Prediction: Rejected (Prob Approved: 0.11, Prob Rejected: 0.89)\n",
            "\n",
            "LIME Explanation (Top Factors contributing to the prediction):\n",
            "Explanation for why the model predicted: 'Approved'\n",
            "\n",
            "--- Human-Understandable Explanation ---\n",
            "The model predicted 'Rejected' for applicant (index 546) primarily because:\n",
            "- The condition 'ApplicantIncome' (Applicant's value for ApplicantIncome: 3358) strongly supported the 'Approved' decision (LIME weight: 0.137).\n",
            "- The condition 'Credit_History' (Applicant's value for Credit_History: 1.0) moderately supported the 'Approved' decision (LIME weight: 0.098).\n",
            "- The condition 'Dependents' (Applicant's value for Dependents: 0.0) slightly supported the 'Approved' decision (LIME weight: 0.019).\n",
            "- The condition 'Self_Employed=0' (Applicant's value for Self_Employed: No) slightly supported the 'Approved' decision (LIME weight: 0.019).\n",
            "- The condition 'Education=1' (Applicant's value for Education: Not Graduate) slightly supported the 'Approved' decision (LIME weight: 0.019).\n",
            "- The condition 'Loan_Amount_Term' (Applicant's value for Loan_Amount_Term: 36.0) slightly opposed the 'Approved' decision (LIME weight: -0.017).\n",
            "- The condition 'LoanAmount' (Applicant's value for LoanAmount: 80.0) slightly supported the 'Approved' decision (LIME weight: 0.014).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Conclusion\n",
        "\n",
        "This notebook demonstrated an end-to-end machine learning workflow for loan eligibility prediction, including data loading, preprocessing, model training, evaluation, and crucially, model explainability using LIME.\n",
        "\n",
        "LIME helps in understanding the \"why\" behind individual predictions, which is vital for sensitive applications like loan approvals to ensure fairness, transparency, and to identify potential issues with the model's reasoning.\n",
        "\n",
        "Further improvements could include:\n",
        "*   More sophisticated feature engineering.\n",
        "*   Hyperparameter tuning for the model.\n",
        "*   Exploring other XAI techniques (e.g., SHAP).\n",
        "*   Developing a more user-friendly interface for presenting these explanations."
      ],
      "metadata": {
        "id": "1zu2PVwTaxoQ"
      }
    }
  ]
}